{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aad1 = ['B004NWLM8K', 'B004Q1NH4U', 'B004LPBTAA']\n",
    "aad2 = ['B004S6NAOU', 'B004R6HTWU', 'B004N8KDNY']\n",
    "aad3 = ['B004KA0RBS', 'B004NPELDA', 'B004L26XXQ']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### this cell is only for declaring functions\n",
    "\n",
    "def readData(filePath): # reads the file and returns a list of the ratings given to a review\n",
    "    ratings = []\n",
    "    identifiers = []\n",
    "    texts = []\n",
    "    file = open(filePath)\n",
    "    lines = file.readlines()\n",
    "    for line in lines:\n",
    "        ratings.append(int(line.split('\\t')[0]))\n",
    "        identifiers.append(line.split('\\t')[1])\n",
    "        texts.append(line.split('\\t')[2])\n",
    "    return ratings, identifiers, texts\n",
    "\n",
    "\n",
    "def giveMeBag(trainData): # creates a count vectorizer and retrns its features bag of words and a list of its words to be used as vocabulary for another count vectorizer\n",
    "    trainingCVect = CountVectorizer(max_features=30000,stop_words='english')\n",
    "    training_bow = trainingCVect.fit_transform(trainData)\n",
    "    training_words = trainingCVect.get_feature_names_out()\n",
    "    return training_bow,training_words\n",
    "\n",
    "def giveMeTreeTrained(training_data,training_labels): # creates and trains a decision tree\n",
    "    dec_tree = DecisionTreeClassifier()\n",
    "    dec_tree.fit(training_data,training_labels)\n",
    "    return dec_tree\n",
    "\n",
    "def giveMeKnn(training_data,training_labels,k): # creates and trains a knn classifier \n",
    "    knnc = KNeighborsClassifier(n_neighbors=k)\n",
    "    knnc.fit(training_data,training_labels)\n",
    "    return knnc\n",
    "\n",
    "def giveMeForest(training_data,training_labels,n): #creates a random forest classifier, trains it and returns the trained forest\n",
    "    forest = RandomForestClassifier(n_estimators=n)\n",
    "    forest.fit(training_data,training_labels)\n",
    "    return forest\n",
    "\n",
    "def giveMeLSVM(training_data,training_label): #creates a linear SVM classifier, trains it and returns the trained classifier\n",
    "    svm = LinearSVC()\n",
    "    svm.fit(training_data,training_label)\n",
    "    return svm\n",
    "\n",
    "def giveMeNLSVM(training_data,training_label): #creates a non linear SVM classifier, trains it and returns the trained classifier\n",
    "    svm = SVC(kernel='rbf')\n",
    "    svm.fit(training_data,training_label)\n",
    "    return svm\n",
    "\n",
    "def giveMeGoodApps(predictedLab,appList): #taking as input the predicted labels of the test data ans the identifiers of the test data, it returns a new list containing only apps with predicted positive sentiment (3)\n",
    "    goodApps = []\n",
    "    counter = 0\n",
    "    for label in predictedLab:\n",
    "        if label == 3:\n",
    "            goodApps.append(appList[counter])\n",
    "        counter += 1\n",
    "    return goodApps\n",
    "\n",
    "def giveMeBadApps(predictedLab,appList): #opposite of giveMeGoodApps\n",
    "    badApps = []\n",
    "    counter = 0\n",
    "    for label in predictedLab:\n",
    "        if label == 1:\n",
    "            badApps.append(appList[counter])\n",
    "        counter += 1\n",
    "    return badApps\n",
    "\n",
    "def giveMeBestDev(appList): # taking in as input all the apps with predicted positive sentiment, this function counts each instance where the identifier of the app is one of our observed 9 and adds 1 to the proper developer company.\n",
    "    aad1Num, aad2Num, aad3Num = 0,0,0\n",
    "    for app in appList:\n",
    "        if app in aad1:\n",
    "            aad1Num += 1\n",
    "        elif app in aad2:\n",
    "            aad2Num += 1\n",
    "        elif app in aad3:\n",
    "            aad3Num += 1\n",
    "    return aad1Num, aad2Num, aad3Num\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instanciations\n",
    "trainingRatings,trainingIdentifiers,trainingReviews = readData('reviews_Apps_for_Android_5.training.txt') # training instances\n",
    "trainingWordsBag,trainingWords = giveMeBag(trainingReviews) # method which creates a training bag of words and vocabulary to use for test\n",
    "\n",
    "testRatings,testIdentifiers,testReviews = readData('reviews_Apps_for_Android_5.test.txt') # reading test data\n",
    "\n",
    "testCV = CountVectorizer(stop_words='english', vocabulary=trainingWords) # create test count vectoriser using the training vocabulary\n",
    "testWordsBag = testCV.fit_transform(testReviews) # create bag of test words to be predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE RESULTS\n",
      "-------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.45      0.41      0.43      3469\n",
      "           2       0.21      0.18      0.19      2087\n",
      "           3       0.81      0.84      0.83     14443\n",
      "\n",
      "    accuracy                           0.70     19999\n",
      "   macro avg       0.49      0.48      0.48     19999\n",
      "weighted avg       0.69      0.70      0.69     19999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DECISION TREE TRAINING\n",
    "trainedTree = giveMeTreeTrained(trainingWordsBag,trainingRatings) # trained tree from training data\n",
    "DTtestPredLabels = trainedTree.predict(testWordsBag) # use the earlier trained decision tree to predict the labels of the test\n",
    "\n",
    "#compute precision, recall and f-measure for each classification label (DECISION TREE)\n",
    "print('DECISION TREE RESULTS\\n-------------------------------------------------------------------------------')\n",
    "print(classification_report(testRatings,DTtestPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN (K=1) RESULTS\n",
      "-------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.34      0.32      3469\n",
      "           2       0.14      0.15      0.15      2087\n",
      "           3       0.78      0.74      0.76     14443\n",
      "\n",
      "    accuracy                           0.61     19999\n",
      "   macro avg       0.41      0.41      0.41     19999\n",
      "weighted avg       0.63      0.61      0.62     19999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN (K=1) TRAINING\n",
    "trainedK = giveMeKnn(trainingWordsBag,trainingRatings,1) # trained tree from training data\n",
    "K1testPredLabels = trainedK.predict(testWordsBag) # use the earlier trained decision tree to predict the labels of the test\n",
    "#compute precision, recall and f-measure for each classification label (DECISION TREE)\n",
    "print('K-NN (K=1) RESULTS\\n-------------------------------------------------------------------------------')\n",
    "print(classification_report(testRatings,K1testPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN (K=3) RESULTS\n",
      "-------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.41      0.35      3469\n",
      "           2       0.16      0.05      0.08      2087\n",
      "           3       0.79      0.79      0.79     14443\n",
      "\n",
      "    accuracy                           0.65     19999\n",
      "   macro avg       0.41      0.42      0.40     19999\n",
      "weighted avg       0.64      0.65      0.64     19999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN (K=3) TRAINING\n",
    "trainedK3 = giveMeKnn(trainingWordsBag,trainingRatings,3) # trained tree from training data\n",
    "K3testPredLabels = trainedK3.predict(testWordsBag) # use the earlier trained decision tree to predict the labels of the test\n",
    "\n",
    "#compute precision, recall and f-measure for each classification label (DECISION TREE)\n",
    "print('K-NN (K=3) RESULTS\\n-------------------------------------------------------------------------------')\n",
    "print(classification_report(testRatings,K3testPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN (K=15) RESULTS\n",
      "-------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.42      0.14      0.21      3469\n",
      "           2       0.23      0.03      0.05      2087\n",
      "           3       0.74      0.96      0.84     14443\n",
      "\n",
      "    accuracy                           0.72     19999\n",
      "   macro avg       0.46      0.37      0.37     19999\n",
      "weighted avg       0.63      0.72      0.65     19999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN (K=15) TRAINING\n",
    "trainedK15 = giveMeKnn(trainingWordsBag,trainingRatings,15) # trained tree from training data\n",
    "K15testPredLabels = trainedK15.predict(testWordsBag) # use the earlier trained decision tree to predict the labels of the test\n",
    "\n",
    "#compute precision, recall and f-measure for each classification label (DECISION TREE)\n",
    "print('K-NN (K=15) RESULTS\\n-------------------------------------------------------------------------------')\n",
    "print(classification_report(testRatings,K15testPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-NN (K=20) RESULTS\n",
      "-------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.46      0.09      0.16      3469\n",
      "           2       0.27      0.02      0.03      2087\n",
      "           3       0.74      0.98      0.84     14443\n",
      "\n",
      "    accuracy                           0.72     19999\n",
      "   macro avg       0.49      0.36      0.34     19999\n",
      "weighted avg       0.64      0.72      0.64     19999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN (K=20) TRAINING\n",
    "trainedK20 = giveMeKnn(trainingWordsBag,trainingRatings,20) # trained tree from training data\n",
    "K20testPredLabels = trainedK20.predict(testWordsBag) # use the earlier trained decision tree to predict the labels of the test\n",
    "\n",
    "#compute precision, recall and f-measure for each classification label (DECISION TREE)\n",
    "print('K-NN (K=20) RESULTS\\n-------------------------------------------------------------------------------')\n",
    "print(classification_report(testRatings,K20testPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM RESULTS\n",
      "-------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.59      0.54      0.56      3469\n",
      "           2       0.24      0.20      0.22      2087\n",
      "           3       0.85      0.88      0.86     14443\n",
      "\n",
      "    accuracy                           0.75     19999\n",
      "   macro avg       0.56      0.54      0.55     19999\n",
      "weighted avg       0.74      0.75      0.74     19999\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeddy Uzodinma\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## Linear SVM\n",
    "lsvm = giveMeLSVM(trainingWordsBag, trainingRatings)\n",
    "LSVMtestPredLabels = lsvm.predict(testWordsBag) # use the earlier trained decision tree to predict the labels of the test\n",
    "\n",
    "#compute precision, recall and f-measure for each classification label\n",
    "print('SVM RESULTS\\n-------------------------------------------------------------------------------')\n",
    "print(classification_report(testRatings,LSVMtestPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Linear SVM RESULTS\n",
      "-------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.42      0.53      3469\n",
      "           2       0.57      0.09      0.16      2087\n",
      "           3       0.80      0.98      0.88     14443\n",
      "\n",
      "    accuracy                           0.79     19999\n",
      "   macro avg       0.70      0.50      0.52     19999\n",
      "weighted avg       0.76      0.79      0.74     19999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Non Linear SVM\n",
    "svm2 = giveMeNLSVM(trainingWordsBag, trainingRatings)\n",
    "NLSVMtestPredLabels = svm2.predict(testWordsBag) # use the earlier trained decision tree to predict the labels of the test\n",
    "\n",
    "#compute precision, recall and f-measure for each classification label\n",
    "print('Non-Linear SVM RESULTS\\n-------------------------------------------------------------------------------')\n",
    "print(classification_report(testRatings,NLSVMtestPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jeddy Uzodinma\\AppData\\Roaming\\Python\\Python313\\site-packages\\sklearn\\svm\\_base.py:1235: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting RESULTS\n",
      "-------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.61      0.35      0.45      3469\n",
      "           2       0.33      0.07      0.12      2087\n",
      "           3       0.78      0.95      0.86     14443\n",
      "\n",
      "    accuracy                           0.76     19999\n",
      "   macro avg       0.57      0.46      0.48     19999\n",
      "weighted avg       0.71      0.76      0.71     19999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Voting based on previous results\n",
    " # Create the voting classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('tree', trainedTree),\n",
    "    ('knn15', trainedK15),\n",
    "    ('knn20', trainedK20),\n",
    "    ('lsvm', lsvm)], voting='hard')\n",
    "\n",
    "# Train the voting classifier on the training data\n",
    "voting_classifier.fit(trainingWordsBag, trainingRatings)\n",
    "\n",
    "# Make predictions on the test data\n",
    "VtestPredLabels = voting_classifier.predict(testWordsBag)\n",
    "\n",
    "#compute precision, recall and f-measure for each classification label (All previous predictions)\n",
    "print('Voting RESULTS\\n-------------------------------------------------------------------------------')\n",
    "print(classification_report(testRatings,VtestPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST RESULTS\n",
      "-------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.67      0.37      0.48      3469\n",
      "           2       0.44      0.07      0.13      2087\n",
      "           3       0.79      0.97      0.87     14443\n",
      "\n",
      "    accuracy                           0.77     19999\n",
      "   macro avg       0.63      0.47      0.49     19999\n",
      "weighted avg       0.73      0.77      0.73     19999\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# random forest classifier\n",
    "trainedForest = giveMeForest(trainingWordsBag,trainingRatings,20) # trained forest from training data\n",
    "RFtestPredLabels = trainedForest.predict(testWordsBag) # use the earlier trained decision tree to predict the labels of the test\n",
    "\n",
    "#compute precision, recall and f-measure for each classification label (DECISION TREE)\n",
    "print('RANDOM FOREST RESULTS\\n-------------------------------------------------------------------------------')\n",
    "print(classification_report(testRatings,RFtestPredLabels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DECISION TREE\n",
      "AAD 1: 89 \n",
      "AAD 2: 186 \n",
      "AAD 3: 88 \n",
      "_______________________________\n",
      "\n",
      "K-NN (K=15)\n",
      "AAD 1: 102 \n",
      "AAD 2: 263 \n",
      "AAD 3: 139 \n",
      "_______________________________\n",
      "\n",
      "K-NN (K=20)\n",
      "AAD 1: 104 \n",
      "AAD 2: 271 \n",
      "AAD 3: 152 \n",
      "_______________________________\n",
      "\n",
      "Linear SVM\n",
      "AAD 1: 97 \n",
      "AAD 2: 155 \n",
      "AAD 3: 75 \n",
      "_______________________________\n",
      "\n",
      "Non-Linear SVM\n",
      "AAD 1: 106 \n",
      "AAD 2: 223 \n",
      "AAD 3: 115 \n",
      "_______________________________\n",
      "\n",
      "Voting Ensemble\n",
      "AAD 1: 102 \n",
      "AAD 2: 222 \n",
      "AAD 3: 114 \n",
      "_______________________________\n",
      "\n",
      "Random Forest\n",
      "AAD 1: 105 \n",
      "AAD 2: 219 \n",
      "AAD 3: 112 \n",
      "_______________________________\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Aggregating the results to find out which apps have best performance and which company has those apps\n",
    "\n",
    "def printDevResults(predictedLabels): # FUNCTION TO AUTOMATICALLY PRINT result of developers\n",
    "    goodApps = giveMeGoodApps(predictedLabels,testIdentifiers) #returns all apps which were predicted to have positive sentiment\n",
    "    a1,a2,a3= giveMeBestDev(goodApps) #returns the number of positive reviews each developer has\n",
    "    print('AAD 1:', a1, '\\nAAD 2:', a2, '\\nAAD 3:', a3, '\\n_______________________________\\n')\n",
    "\n",
    "# def printDevResults(predictedLabels): # FUNCTION TO AUTOMATICALLY PRINT result of developers\n",
    "#     goodApps = giveMeBadApps(predictedLabels,testIdentifiers) #returns all apps which were predicted to have positive sentiment\n",
    "#     a1,a2,a3= giveMeBestDev(goodApps) #returns the number of positive reviews each developer has\n",
    "#     print('AAD 1:', a1, '\\nAAD 2:', a2, '\\nAAD 3:', a3, '\\n_______________________________\\n')\n",
    "\n",
    "print('DECISION TREE')\n",
    "printDevResults(DTtestPredLabels)\n",
    "\n",
    "print('K-NN (K=15)')\n",
    "printDevResults(K15testPredLabels)\n",
    "\n",
    "print('K-NN (K=20)')\n",
    "printDevResults(K20testPredLabels)\n",
    "\n",
    "print('Linear SVM')\n",
    "printDevResults(LSVMtestPredLabels)\n",
    "\n",
    "print('Non-Linear SVM')\n",
    "printDevResults(NLSVMtestPredLabels)\n",
    "\n",
    "print('Voting Ensemble')\n",
    "printDevResults(VtestPredLabels)\n",
    "\n",
    "print('Random Forest')\n",
    "printDevResults(RFtestPredLabels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
